{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prerequisites\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images shape: (60000, 28, 28)\n",
      "Train labels shape: (60000,)\n",
      "Test images shape: (10000, 28, 28)\n",
      "Test labels shape: (10000,)\n",
      "Train images tensor shape: torch.Size([60000, 28, 28])\n",
      "Train images tensor range: [0.000, 1.000]\n"
     ]
    }
   ],
   "source": [
    "# Alternative: Load raw MNIST data directly\n",
    "import struct\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def load_mnist_images(filename):\n",
    "    \"\"\"Load MNIST images from raw binary file\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        magic, num, rows, cols = struct.unpack('>IIII', f.read(16))\n",
    "        images = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        images = images.reshape(num, rows, cols)\n",
    "    return images\n",
    "\n",
    "def load_mnist_labels(filename):\n",
    "    \"\"\"Load MNIST labels from raw binary file\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        magic, num = struct.unpack('>II', f.read(8))\n",
    "        labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "    return labels\n",
    "\n",
    "# Load raw data\n",
    "train_images = load_mnist_images('./mnist_data/raw/train-images-idx3-ubyte')\n",
    "train_labels = load_mnist_labels('./mnist_data/raw/train-labels-idx1-ubyte')\n",
    "test_images = load_mnist_images('./mnist_data/raw/t10k-images-idx3-ubyte')\n",
    "test_labels = load_mnist_labels('./mnist_data/raw/t10k-labels-idx1-ubyte')\n",
    "\n",
    "print(f\"Train images shape: {train_images.shape}\")\n",
    "print(f\"Train labels shape: {train_labels.shape}\")\n",
    "print(f\"Test images shape: {test_images.shape}\")\n",
    "print(f\"Test labels shape: {test_labels.shape}\")\n",
    "\n",
    "# Convert to PyTorch tensors and normalize\n",
    "train_images_tensor = torch.FloatTensor(train_images) / 255.0\n",
    "test_images_tensor = torch.FloatTensor(test_images) / 255.0\n",
    "train_labels_tensor = torch.LongTensor(train_labels)\n",
    "test_labels_tensor = torch.LongTensor(test_labels)\n",
    "\n",
    "print(f\"Train images tensor shape: {train_images_tensor.shape}\")\n",
    "print(f\"Train images tensor range: [{train_images_tensor.min():.3f}, {train_images_tensor.max():.3f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original train labels shape: torch.Size([60000])\n",
      "One-hot train labels shape: torch.Size([60000, 10])\n",
      "Original train labels (first 5): tensor([5, 0, 4, 1, 9])\n",
      "One-hot train labels (first 5):\n",
      "tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n",
      "\n",
      "Manual one-hot train labels shape: torch.Size([60000, 10])\n",
      "Manual one-hot train labels (first 5):\n",
      "tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n",
      "\n",
      "Methods are equivalent: True\n"
     ]
    }
   ],
   "source": [
    "# Convert labels to one-hot encoding\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Method 1: Using torch.nn.functional.one_hot (PyTorch 1.6+)\n",
    "train_labels_onehot = F.one_hot(train_labels_tensor, num_classes=10).float()\n",
    "test_labels_onehot = F.one_hot(test_labels_tensor, num_classes=10).float()\n",
    "\n",
    "print(f\"Original train labels shape: {train_labels_tensor.shape}\")\n",
    "print(f\"One-hot train labels shape: {train_labels_onehot.shape}\")\n",
    "print(f\"Original train labels (first 5): {train_labels_tensor[:5]}\")\n",
    "print(f\"One-hot train labels (first 5):\")\n",
    "print(train_labels_onehot[:5])\n",
    "\n",
    "# Method 2: Manual one-hot encoding\n",
    "def to_one_hot(labels, num_classes=10):\n",
    "    \"\"\"Convert labels to one-hot encoding manually\"\"\"\n",
    "    one_hot = torch.zeros(labels.size(0), num_classes)\n",
    "    one_hot.scatter_(1, labels.unsqueeze(1), 1)\n",
    "    return one_hot\n",
    "\n",
    "# Alternative manual method\n",
    "train_labels_onehot_manual = to_one_hot(train_labels_tensor)\n",
    "test_labels_onehot_manual = to_one_hot(test_labels_tensor)\n",
    "\n",
    "print(f\"\\nManual one-hot train labels shape: {train_labels_onehot_manual.shape}\")\n",
    "print(f\"Manual one-hot train labels (first 5):\")\n",
    "print(train_labels_onehot_manual[:5])\n",
    "\n",
    "# Verify both methods give same result\n",
    "print(f\"\\nMethods are equivalent: {torch.equal(train_labels_onehot, train_labels_onehot_manual)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original train images shape: torch.Size([60000, 784])\n",
      "One-hot train labels shape: torch.Size([60000, 10])\n",
      "Concatenated train data shape: torch.Size([60000, 794])\n",
      "Concatenated test data shape: torch.Size([10000, 794])\n",
      "\n",
      "First sample - Image part (first 5 pixels): tensor([0., 0., 0., 0., 0.])\n",
      "First sample - Label part (last 10 values): tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "Original label for first sample: 5\n",
      "\n",
      "Train loader batches: 600\n",
      "Test loader batches: 100\n",
      "\n",
      "Batch data shape: torch.Size([100, 794])\n",
      "Batch labels shape: torch.Size([100])\n",
      "First sample in batch - Image part: tensor([0., 0., 0., 0., 0.])\n",
      "First sample in batch - Label part: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "First sample original label: 5\n"
     ]
    }
   ],
   "source": [
    "# Concatenate one-hot labels with image data for conditional generation\n",
    "import torch\n",
    "\n",
    "# Reshape images to flat vectors (784 dimensions for 28x28 images)\n",
    "train_images_flat = train_images_tensor.view(-1, 784)  # (60000, 784)\n",
    "test_images_flat = test_images_tensor.view(-1, 784)    # (10000, 784)\n",
    "\n",
    "# Concatenate images with one-hot labels\n",
    "train_conditional_data = torch.cat([train_images_flat, train_labels_onehot], dim=1)  # (60000, 794)\n",
    "test_conditional_data = torch.cat([test_images_flat, test_labels_onehot], dim=1)     # (10000, 794)\n",
    "\n",
    "print(f\"Original train images shape: {train_images_flat.shape}\")\n",
    "print(f\"One-hot train labels shape: {train_labels_onehot.shape}\")\n",
    "print(f\"Concatenated train data shape: {train_conditional_data.shape}\")\n",
    "print(f\"Concatenated test data shape: {test_conditional_data.shape}\")\n",
    "\n",
    "# Verify concatenation\n",
    "print(f\"\\nFirst sample - Image part (first 5 pixels): {train_conditional_data[0, :5]}\")\n",
    "print(f\"First sample - Label part (last 10 values): {train_conditional_data[0, -10:]}\")\n",
    "print(f\"Original label for first sample: {train_labels_tensor[0]}\")\n",
    "\n",
    "# Create DataLoaders for conditional data\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Create datasets\n",
    "train_dataset_conditional = TensorDataset(train_conditional_data, train_labels_tensor)\n",
    "test_dataset_conditional = TensorDataset(test_conditional_data, test_labels_tensor)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader_conditional = DataLoader(train_dataset_conditional, batch_size=100, shuffle=True)\n",
    "test_loader_conditional = DataLoader(test_dataset_conditional, batch_size=100, shuffle=False)\n",
    "\n",
    "print(f\"\\nTrain loader batches: {len(train_loader_conditional)}\")\n",
    "print(f\"Test loader batches: {len(test_loader_conditional)}\")\n",
    "\n",
    "# Example: Get a batch and verify structure\n",
    "for batch_data, batch_labels in train_loader_conditional:\n",
    "    print(f\"\\nBatch data shape: {batch_data.shape}\")\n",
    "    print(f\"Batch labels shape: {batch_labels.shape}\")\n",
    "    print(f\"First sample in batch - Image part: {batch_data[0, :5]}\")\n",
    "    print(f\"First sample in batch - Label part: {batch_data[0, -10:]}\")\n",
    "    print(f\"First sample original label: {batch_labels[0]}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalVAE(nn.Module):\n",
    "    def __init__(self, x_dim, label_dim, h_dim1, h_dim2, z_dim):\n",
    "        super(ConditionalVAE, self).__init__()\n",
    "        \n",
    "        # encoder part - takes concatenated image + label\n",
    "        self.fc1 = nn.Linear(x_dim + label_dim, h_dim1)  # 784 + 10 = 794\n",
    "        self.fc2 = nn.Linear(h_dim1, h_dim2)\n",
    "        self.fc31 = nn.Linear(h_dim2, z_dim)\n",
    "        self.fc32 = nn.Linear(h_dim2, z_dim)\n",
    "        \n",
    "        # decoder part - takes z + label for conditional generation\n",
    "        self.fc4 = nn.Linear(z_dim + label_dim, h_dim2)  # z + label\n",
    "        self.fc5 = nn.Linear(h_dim2, h_dim1)\n",
    "        self.fc6 = nn.Linear(h_dim1, x_dim)  # output only image (784)\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return self.fc31(h), self.fc32(h) # mu, log_var\n",
    "    \n",
    "    def sampling(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu) # return z sample\n",
    "        \n",
    "    def decoder(self, z, labels):\n",
    "        # Concatenate z with labels for conditional generation\n",
    "        z_with_labels = torch.cat([z, labels], dim=1)\n",
    "        h = F.relu(self.fc4(z_with_labels))\n",
    "        h = F.relu(self.fc5(h))\n",
    "        return torch.sigmoid(self.fc6(h))  # output only image part\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encoder(x)  # x is already 794-dimensional\n",
    "        z = self.sampling(mu, log_var)\n",
    "        \n",
    "        # Separate image and label parts\n",
    "        images = x[:, :784]  # first 784 dimensions\n",
    "        labels = x[:, 784:]  # last 10 dimensions\n",
    "        \n",
    "        recon_images = self.decoder(z, labels)\n",
    "        return recon_images, mu, log_var\n",
    "\n",
    "# build conditional model\n",
    "vae = ConditionalVAE(x_dim=784, label_dim=10, h_dim1=512, h_dim2=256, z_dim=2)\n",
    "if torch.cuda.is_available():\n",
    "    vae.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConditionalVAE(\n",
       "  (fc1): Linear(in_features=794, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc31): Linear(in_features=256, out_features=2, bias=True)\n",
       "  (fc32): Linear(in_features=256, out_features=2, bias=True)\n",
       "  (fc4): Linear(in_features=12, out_features=256, bias=True)\n",
       "  (fc5): Linear(in_features=256, out_features=512, bias=True)\n",
       "  (fc6): Linear(in_features=512, out_features=784, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(vae.parameters())\n",
    "# return reconstruction error + KL divergence losses\n",
    "def loss_function(recon_images, x, mu, log_var):\n",
    "    # Extract only the image part (first 784 dimensions) for reconstruction loss\n",
    "    target_images = x[:, :784]  # first 784 dimensions are images\n",
    "    BCE = F.binary_cross_entropy(recon_images, target_images, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader_conditional):\n",
    "        data = data.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        recon_batch, mu, log_var = vae(data)\n",
    "        loss = loss_function(recon_batch, data, mu, log_var)\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader_conditional.dataset),\n",
    "                100. * batch_idx / len(train_loader_conditional), loss.item() / len(data)))\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader_conditional.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    vae.eval()\n",
    "    test_loss= 0\n",
    "    with torch.no_grad():\n",
    "        for data, _ in test_loader_conditional:\n",
    "            data = data.cuda()\n",
    "            recon, mu, log_var = vae(data)\n",
    "            \n",
    "            # sum up batch loss\n",
    "            test_loss += loss_function(recon, data, mu, log_var).item()\n",
    "        \n",
    "    test_loss /= len(test_loader_conditional.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 542.882305\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 173.287969\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 152.160596\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 155.495117\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 144.250859\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 138.256504\n",
      "====> Epoch: 1 Average loss: 160.9827\n",
      "====> Test set loss: 141.0363\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 142.153477\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 152.939443\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 129.381240\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 136.885283\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 136.572979\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 137.717471\n",
      "====> Epoch: 2 Average loss: 138.3065\n",
      "====> Test set loss: 136.4774\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 137.669443\n",
      "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 137.031113\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 139.428311\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 142.119736\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 134.449668\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 141.840352\n",
      "====> Epoch: 3 Average loss: 135.0846\n",
      "====> Test set loss: 134.3047\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 135.532217\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 129.412295\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 123.638975\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 127.890479\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 134.953701\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 131.943252\n",
      "====> Epoch: 4 Average loss: 133.3477\n",
      "====> Test set loss: 133.0705\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 128.644385\n",
      "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 140.751162\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 130.643945\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 128.987607\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 133.419580\n",
      "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 124.514268\n",
      "====> Epoch: 5 Average loss: 132.2238\n",
      "====> Test set loss: 132.1766\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 129.524307\n",
      "Train Epoch: 6 [10000/60000 (17%)]\tLoss: 132.291309\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 130.962510\n",
      "Train Epoch: 6 [30000/60000 (50%)]\tLoss: 126.388437\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 132.729805\n",
      "Train Epoch: 6 [50000/60000 (83%)]\tLoss: 129.302969\n",
      "====> Epoch: 6 Average loss: 131.4938\n",
      "====> Test set loss: 132.0188\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 137.548574\n",
      "Train Epoch: 7 [10000/60000 (17%)]\tLoss: 135.425898\n",
      "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 130.549092\n",
      "Train Epoch: 7 [30000/60000 (50%)]\tLoss: 126.852979\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 131.646182\n",
      "Train Epoch: 7 [50000/60000 (83%)]\tLoss: 131.689521\n",
      "====> Epoch: 7 Average loss: 130.9228\n",
      "====> Test set loss: 131.5359\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 128.767891\n",
      "Train Epoch: 8 [10000/60000 (17%)]\tLoss: 133.119863\n",
      "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 133.540586\n",
      "Train Epoch: 8 [30000/60000 (50%)]\tLoss: 129.991514\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 126.099307\n",
      "Train Epoch: 8 [50000/60000 (83%)]\tLoss: 133.873369\n",
      "====> Epoch: 8 Average loss: 130.4381\n",
      "====> Test set loss: 130.9532\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 132.245732\n",
      "Train Epoch: 9 [10000/60000 (17%)]\tLoss: 130.010146\n",
      "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 135.072246\n",
      "Train Epoch: 9 [30000/60000 (50%)]\tLoss: 132.921777\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 123.546562\n",
      "Train Epoch: 9 [50000/60000 (83%)]\tLoss: 131.812402\n",
      "====> Epoch: 9 Average loss: 130.0297\n",
      "====> Test set loss: 130.4505\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 125.173926\n",
      "Train Epoch: 10 [10000/60000 (17%)]\tLoss: 121.603623\n",
      "Train Epoch: 10 [20000/60000 (33%)]\tLoss: 129.913223\n",
      "Train Epoch: 10 [30000/60000 (50%)]\tLoss: 131.638867\n",
      "Train Epoch: 10 [40000/60000 (67%)]\tLoss: 137.606445\n",
      "Train Epoch: 10 [50000/60000 (83%)]\tLoss: 129.219248\n",
      "====> Epoch: 10 Average loss: 129.6611\n",
      "====> Test set loss: 130.4030\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 129.614492\n",
      "Train Epoch: 11 [10000/60000 (17%)]\tLoss: 133.775645\n",
      "Train Epoch: 11 [20000/60000 (33%)]\tLoss: 131.070361\n",
      "Train Epoch: 11 [30000/60000 (50%)]\tLoss: 130.881143\n",
      "Train Epoch: 11 [40000/60000 (67%)]\tLoss: 125.120049\n",
      "Train Epoch: 11 [50000/60000 (83%)]\tLoss: 127.172617\n",
      "====> Epoch: 11 Average loss: 129.3520\n",
      "====> Test set loss: 130.0123\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 136.553799\n",
      "Train Epoch: 12 [10000/60000 (17%)]\tLoss: 133.283828\n",
      "Train Epoch: 12 [20000/60000 (33%)]\tLoss: 125.449180\n",
      "Train Epoch: 12 [30000/60000 (50%)]\tLoss: 127.290566\n",
      "Train Epoch: 12 [40000/60000 (67%)]\tLoss: 124.576533\n",
      "Train Epoch: 12 [50000/60000 (83%)]\tLoss: 129.471582\n",
      "====> Epoch: 12 Average loss: 129.0826\n",
      "====> Test set loss: 129.8942\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 130.432090\n",
      "Train Epoch: 13 [10000/60000 (17%)]\tLoss: 131.373018\n",
      "Train Epoch: 13 [20000/60000 (33%)]\tLoss: 126.342617\n",
      "Train Epoch: 13 [30000/60000 (50%)]\tLoss: 129.698711\n",
      "Train Epoch: 13 [40000/60000 (67%)]\tLoss: 133.216719\n",
      "Train Epoch: 13 [50000/60000 (83%)]\tLoss: 123.149766\n",
      "====> Epoch: 13 Average loss: 128.7928\n",
      "====> Test set loss: 129.7332\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 129.708252\n",
      "Train Epoch: 14 [10000/60000 (17%)]\tLoss: 134.174492\n",
      "Train Epoch: 14 [20000/60000 (33%)]\tLoss: 125.650801\n",
      "Train Epoch: 14 [30000/60000 (50%)]\tLoss: 137.383887\n",
      "Train Epoch: 14 [40000/60000 (67%)]\tLoss: 135.254043\n",
      "Train Epoch: 14 [50000/60000 (83%)]\tLoss: 136.018877\n",
      "====> Epoch: 14 Average loss: 128.5410\n",
      "====> Test set loss: 129.4924\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 133.429697\n",
      "Train Epoch: 15 [10000/60000 (17%)]\tLoss: 129.507480\n",
      "Train Epoch: 15 [20000/60000 (33%)]\tLoss: 128.386025\n",
      "Train Epoch: 15 [30000/60000 (50%)]\tLoss: 121.693418\n",
      "Train Epoch: 15 [40000/60000 (67%)]\tLoss: 128.376621\n",
      "Train Epoch: 15 [50000/60000 (83%)]\tLoss: 124.097930\n",
      "====> Epoch: 15 Average loss: 128.2835\n",
      "====> Test set loss: 129.4395\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 127.143545\n",
      "Train Epoch: 16 [10000/60000 (17%)]\tLoss: 124.538779\n",
      "Train Epoch: 16 [20000/60000 (33%)]\tLoss: 130.768184\n",
      "Train Epoch: 16 [30000/60000 (50%)]\tLoss: 136.071562\n",
      "Train Epoch: 16 [40000/60000 (67%)]\tLoss: 125.525312\n",
      "Train Epoch: 16 [50000/60000 (83%)]\tLoss: 126.050938\n",
      "====> Epoch: 16 Average loss: 128.0846\n",
      "====> Test set loss: 129.2453\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 122.213447\n",
      "Train Epoch: 17 [10000/60000 (17%)]\tLoss: 122.836162\n",
      "Train Epoch: 17 [20000/60000 (33%)]\tLoss: 122.643350\n",
      "Train Epoch: 17 [30000/60000 (50%)]\tLoss: 125.663242\n",
      "Train Epoch: 17 [40000/60000 (67%)]\tLoss: 129.212998\n",
      "Train Epoch: 17 [50000/60000 (83%)]\tLoss: 130.134688\n",
      "====> Epoch: 17 Average loss: 127.7882\n",
      "====> Test set loss: 128.9817\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 121.621875\n",
      "Train Epoch: 18 [10000/60000 (17%)]\tLoss: 132.315508\n",
      "Train Epoch: 18 [20000/60000 (33%)]\tLoss: 130.983428\n",
      "Train Epoch: 18 [30000/60000 (50%)]\tLoss: 127.446279\n",
      "Train Epoch: 18 [40000/60000 (67%)]\tLoss: 131.685771\n",
      "Train Epoch: 18 [50000/60000 (83%)]\tLoss: 128.970586\n",
      "====> Epoch: 18 Average loss: 127.5995\n",
      "====> Test set loss: 128.7931\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 120.992334\n",
      "Train Epoch: 19 [10000/60000 (17%)]\tLoss: 134.170166\n",
      "Train Epoch: 19 [20000/60000 (33%)]\tLoss: 129.594600\n",
      "Train Epoch: 19 [30000/60000 (50%)]\tLoss: 129.875771\n",
      "Train Epoch: 19 [40000/60000 (67%)]\tLoss: 125.041562\n",
      "Train Epoch: 19 [50000/60000 (83%)]\tLoss: 130.062822\n",
      "====> Epoch: 19 Average loss: 127.3498\n",
      "====> Test set loss: 129.1008\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 127.993457\n",
      "Train Epoch: 20 [10000/60000 (17%)]\tLoss: 129.416855\n",
      "Train Epoch: 20 [20000/60000 (33%)]\tLoss: 130.999424\n",
      "Train Epoch: 20 [30000/60000 (50%)]\tLoss: 123.271523\n",
      "Train Epoch: 20 [40000/60000 (67%)]\tLoss: 125.726650\n",
      "Train Epoch: 20 [50000/60000 (83%)]\tLoss: 125.533057\n",
      "====> Epoch: 20 Average loss: 127.1596\n",
      "====> Test set loss: 128.7607\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 121.639727\n",
      "Train Epoch: 21 [10000/60000 (17%)]\tLoss: 124.970732\n",
      "Train Epoch: 21 [20000/60000 (33%)]\tLoss: 116.812393\n",
      "Train Epoch: 21 [30000/60000 (50%)]\tLoss: 124.322568\n",
      "Train Epoch: 21 [40000/60000 (67%)]\tLoss: 129.372227\n",
      "Train Epoch: 21 [50000/60000 (83%)]\tLoss: 122.993242\n",
      "====> Epoch: 21 Average loss: 126.9438\n",
      "====> Test set loss: 128.8279\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 124.997217\n",
      "Train Epoch: 22 [10000/60000 (17%)]\tLoss: 123.639521\n",
      "Train Epoch: 22 [20000/60000 (33%)]\tLoss: 136.340996\n",
      "Train Epoch: 22 [30000/60000 (50%)]\tLoss: 132.678730\n",
      "Train Epoch: 22 [40000/60000 (67%)]\tLoss: 117.462432\n",
      "Train Epoch: 22 [50000/60000 (83%)]\tLoss: 131.110391\n",
      "====> Epoch: 22 Average loss: 126.7512\n",
      "====> Test set loss: 128.7466\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 117.774072\n",
      "Train Epoch: 23 [10000/60000 (17%)]\tLoss: 130.493535\n",
      "Train Epoch: 23 [20000/60000 (33%)]\tLoss: 128.293047\n",
      "Train Epoch: 23 [30000/60000 (50%)]\tLoss: 125.157988\n",
      "Train Epoch: 23 [40000/60000 (67%)]\tLoss: 116.717773\n",
      "Train Epoch: 23 [50000/60000 (83%)]\tLoss: 121.242432\n",
      "====> Epoch: 23 Average loss: 126.5669\n",
      "====> Test set loss: 128.5765\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 131.058066\n",
      "Train Epoch: 24 [10000/60000 (17%)]\tLoss: 123.062139\n",
      "Train Epoch: 24 [20000/60000 (33%)]\tLoss: 127.642549\n",
      "Train Epoch: 24 [30000/60000 (50%)]\tLoss: 121.656621\n",
      "Train Epoch: 24 [40000/60000 (67%)]\tLoss: 125.429688\n",
      "Train Epoch: 24 [50000/60000 (83%)]\tLoss: 119.768809\n",
      "====> Epoch: 24 Average loss: 126.3587\n",
      "====> Test set loss: 128.3458\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 123.514014\n",
      "Train Epoch: 25 [10000/60000 (17%)]\tLoss: 124.222197\n",
      "Train Epoch: 25 [20000/60000 (33%)]\tLoss: 125.098008\n",
      "Train Epoch: 25 [30000/60000 (50%)]\tLoss: 119.072471\n",
      "Train Epoch: 25 [40000/60000 (67%)]\tLoss: 130.513076\n",
      "Train Epoch: 25 [50000/60000 (83%)]\tLoss: 124.520361\n",
      "====> Epoch: 25 Average loss: 126.1835\n",
      "====> Test set loss: 128.3564\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 125.291279\n",
      "Train Epoch: 26 [10000/60000 (17%)]\tLoss: 125.549404\n",
      "Train Epoch: 26 [20000/60000 (33%)]\tLoss: 125.106211\n",
      "Train Epoch: 26 [30000/60000 (50%)]\tLoss: 125.205039\n",
      "Train Epoch: 26 [40000/60000 (67%)]\tLoss: 123.012363\n",
      "Train Epoch: 26 [50000/60000 (83%)]\tLoss: 130.138701\n",
      "====> Epoch: 26 Average loss: 126.0303\n",
      "====> Test set loss: 128.4634\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 127.647451\n",
      "Train Epoch: 27 [10000/60000 (17%)]\tLoss: 124.538848\n",
      "Train Epoch: 27 [20000/60000 (33%)]\tLoss: 119.877910\n",
      "Train Epoch: 27 [30000/60000 (50%)]\tLoss: 129.326123\n",
      "Train Epoch: 27 [40000/60000 (67%)]\tLoss: 128.735713\n",
      "Train Epoch: 27 [50000/60000 (83%)]\tLoss: 124.110195\n",
      "====> Epoch: 27 Average loss: 125.8760\n",
      "====> Test set loss: 128.3116\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 123.731943\n",
      "Train Epoch: 28 [10000/60000 (17%)]\tLoss: 123.393896\n",
      "Train Epoch: 28 [20000/60000 (33%)]\tLoss: 130.901807\n",
      "Train Epoch: 28 [30000/60000 (50%)]\tLoss: 126.095166\n",
      "Train Epoch: 28 [40000/60000 (67%)]\tLoss: 123.614775\n",
      "Train Epoch: 28 [50000/60000 (83%)]\tLoss: 135.358730\n",
      "====> Epoch: 28 Average loss: 125.7648\n",
      "====> Test set loss: 128.2317\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 120.658330\n",
      "Train Epoch: 29 [10000/60000 (17%)]\tLoss: 126.955469\n",
      "Train Epoch: 29 [20000/60000 (33%)]\tLoss: 124.124023\n",
      "Train Epoch: 29 [30000/60000 (50%)]\tLoss: 130.762295\n",
      "Train Epoch: 29 [40000/60000 (67%)]\tLoss: 128.112334\n",
      "Train Epoch: 29 [50000/60000 (83%)]\tLoss: 118.810039\n",
      "====> Epoch: 29 Average loss: 125.5525\n",
      "====> Test set loss: 128.0145\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 123.999658\n",
      "Train Epoch: 30 [10000/60000 (17%)]\tLoss: 119.574102\n",
      "Train Epoch: 30 [20000/60000 (33%)]\tLoss: 124.411592\n",
      "Train Epoch: 30 [30000/60000 (50%)]\tLoss: 124.220205\n",
      "Train Epoch: 30 [40000/60000 (67%)]\tLoss: 127.307520\n",
      "Train Epoch: 30 [50000/60000 (83%)]\tLoss: 122.571387\n",
      "====> Epoch: 30 Average loss: 125.4092\n",
      "====> Test set loss: 128.0097\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: 119.042002\n",
      "Train Epoch: 31 [10000/60000 (17%)]\tLoss: 124.938174\n",
      "Train Epoch: 31 [20000/60000 (33%)]\tLoss: 126.459170\n",
      "Train Epoch: 31 [30000/60000 (50%)]\tLoss: 124.414453\n",
      "Train Epoch: 31 [40000/60000 (67%)]\tLoss: 131.786201\n",
      "Train Epoch: 31 [50000/60000 (83%)]\tLoss: 127.815664\n",
      "====> Epoch: 31 Average loss: 125.2723\n",
      "====> Test set loss: 128.1156\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: 126.121172\n",
      "Train Epoch: 32 [10000/60000 (17%)]\tLoss: 122.506826\n",
      "Train Epoch: 32 [20000/60000 (33%)]\tLoss: 118.379932\n",
      "Train Epoch: 32 [30000/60000 (50%)]\tLoss: 131.640361\n",
      "Train Epoch: 32 [40000/60000 (67%)]\tLoss: 125.719902\n",
      "Train Epoch: 32 [50000/60000 (83%)]\tLoss: 129.398506\n",
      "====> Epoch: 32 Average loss: 125.1225\n",
      "====> Test set loss: 127.9570\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: 118.521650\n",
      "Train Epoch: 33 [10000/60000 (17%)]\tLoss: 121.344170\n",
      "Train Epoch: 33 [20000/60000 (33%)]\tLoss: 129.791289\n",
      "Train Epoch: 33 [30000/60000 (50%)]\tLoss: 128.485674\n",
      "Train Epoch: 33 [40000/60000 (67%)]\tLoss: 126.644395\n",
      "Train Epoch: 33 [50000/60000 (83%)]\tLoss: 127.784160\n",
      "====> Epoch: 33 Average loss: 124.9951\n",
      "====> Test set loss: 128.0270\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: 121.963828\n",
      "Train Epoch: 34 [10000/60000 (17%)]\tLoss: 131.351309\n",
      "Train Epoch: 34 [20000/60000 (33%)]\tLoss: 126.418223\n",
      "Train Epoch: 34 [30000/60000 (50%)]\tLoss: 131.622822\n",
      "Train Epoch: 34 [40000/60000 (67%)]\tLoss: 122.678916\n",
      "Train Epoch: 34 [50000/60000 (83%)]\tLoss: 119.596172\n",
      "====> Epoch: 34 Average loss: 124.8448\n",
      "====> Test set loss: 127.8990\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 120.373066\n",
      "Train Epoch: 35 [10000/60000 (17%)]\tLoss: 132.026240\n",
      "Train Epoch: 35 [20000/60000 (33%)]\tLoss: 125.322441\n",
      "Train Epoch: 35 [30000/60000 (50%)]\tLoss: 124.535693\n",
      "Train Epoch: 35 [40000/60000 (67%)]\tLoss: 126.125098\n",
      "Train Epoch: 35 [50000/60000 (83%)]\tLoss: 122.045713\n",
      "====> Epoch: 35 Average loss: 124.7425\n",
      "====> Test set loss: 127.8661\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: 129.790312\n",
      "Train Epoch: 36 [10000/60000 (17%)]\tLoss: 111.857637\n",
      "Train Epoch: 36 [20000/60000 (33%)]\tLoss: 132.032832\n",
      "Train Epoch: 36 [30000/60000 (50%)]\tLoss: 128.421367\n",
      "Train Epoch: 36 [40000/60000 (67%)]\tLoss: 125.679785\n",
      "Train Epoch: 36 [50000/60000 (83%)]\tLoss: 131.820654\n",
      "====> Epoch: 36 Average loss: 124.6204\n",
      "====> Test set loss: 127.9261\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: 122.303574\n",
      "Train Epoch: 37 [10000/60000 (17%)]\tLoss: 122.528594\n",
      "Train Epoch: 37 [20000/60000 (33%)]\tLoss: 132.133887\n",
      "Train Epoch: 37 [30000/60000 (50%)]\tLoss: 120.282334\n",
      "Train Epoch: 37 [40000/60000 (67%)]\tLoss: 129.653574\n",
      "Train Epoch: 37 [50000/60000 (83%)]\tLoss: 132.296445\n",
      "====> Epoch: 37 Average loss: 124.4915\n",
      "====> Test set loss: 127.7730\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: 124.858916\n",
      "Train Epoch: 38 [10000/60000 (17%)]\tLoss: 120.310752\n",
      "Train Epoch: 38 [20000/60000 (33%)]\tLoss: 117.923037\n",
      "Train Epoch: 38 [30000/60000 (50%)]\tLoss: 125.348262\n",
      "Train Epoch: 38 [40000/60000 (67%)]\tLoss: 126.463906\n",
      "Train Epoch: 38 [50000/60000 (83%)]\tLoss: 122.724766\n",
      "====> Epoch: 38 Average loss: 124.4499\n",
      "====> Test set loss: 127.8623\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: 130.443652\n",
      "Train Epoch: 39 [10000/60000 (17%)]\tLoss: 122.002969\n",
      "Train Epoch: 39 [20000/60000 (33%)]\tLoss: 126.996650\n",
      "Train Epoch: 39 [30000/60000 (50%)]\tLoss: 117.733730\n",
      "Train Epoch: 39 [40000/60000 (67%)]\tLoss: 118.601504\n",
      "Train Epoch: 39 [50000/60000 (83%)]\tLoss: 117.279697\n",
      "====> Epoch: 39 Average loss: 124.2287\n",
      "====> Test set loss: 127.7670\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 121.124521\n",
      "Train Epoch: 40 [10000/60000 (17%)]\tLoss: 130.588516\n",
      "Train Epoch: 40 [20000/60000 (33%)]\tLoss: 122.990537\n",
      "Train Epoch: 40 [30000/60000 (50%)]\tLoss: 121.368896\n",
      "Train Epoch: 40 [40000/60000 (67%)]\tLoss: 114.999980\n",
      "Train Epoch: 40 [50000/60000 (83%)]\tLoss: 127.979473\n",
      "====> Epoch: 40 Average loss: 124.1796\n",
      "====> Test set loss: 127.8817\n",
      "Train Epoch: 41 [0/60000 (0%)]\tLoss: 124.530039\n",
      "Train Epoch: 41 [10000/60000 (17%)]\tLoss: 121.267676\n",
      "Train Epoch: 41 [20000/60000 (33%)]\tLoss: 119.824502\n",
      "Train Epoch: 41 [30000/60000 (50%)]\tLoss: 126.397539\n",
      "Train Epoch: 41 [40000/60000 (67%)]\tLoss: 123.815117\n",
      "Train Epoch: 41 [50000/60000 (83%)]\tLoss: 124.453115\n",
      "====> Epoch: 41 Average loss: 124.0403\n",
      "====> Test set loss: 127.7407\n",
      "Train Epoch: 42 [0/60000 (0%)]\tLoss: 121.596475\n",
      "Train Epoch: 42 [10000/60000 (17%)]\tLoss: 119.937607\n",
      "Train Epoch: 42 [20000/60000 (33%)]\tLoss: 125.368047\n",
      "Train Epoch: 42 [30000/60000 (50%)]\tLoss: 129.718613\n",
      "Train Epoch: 42 [40000/60000 (67%)]\tLoss: 118.228311\n",
      "Train Epoch: 42 [50000/60000 (83%)]\tLoss: 119.986348\n",
      "====> Epoch: 42 Average loss: 123.9369\n",
      "====> Test set loss: 127.8641\n",
      "Train Epoch: 43 [0/60000 (0%)]\tLoss: 130.221641\n",
      "Train Epoch: 43 [10000/60000 (17%)]\tLoss: 127.605156\n",
      "Train Epoch: 43 [20000/60000 (33%)]\tLoss: 125.859424\n",
      "Train Epoch: 43 [30000/60000 (50%)]\tLoss: 130.434033\n",
      "Train Epoch: 43 [40000/60000 (67%)]\tLoss: 125.583135\n",
      "Train Epoch: 43 [50000/60000 (83%)]\tLoss: 123.178613\n",
      "====> Epoch: 43 Average loss: 123.8804\n",
      "====> Test set loss: 127.7892\n",
      "Train Epoch: 44 [0/60000 (0%)]\tLoss: 123.104600\n",
      "Train Epoch: 44 [10000/60000 (17%)]\tLoss: 124.679727\n",
      "Train Epoch: 44 [20000/60000 (33%)]\tLoss: 119.664121\n",
      "Train Epoch: 44 [30000/60000 (50%)]\tLoss: 133.162979\n",
      "Train Epoch: 44 [40000/60000 (67%)]\tLoss: 127.389531\n",
      "Train Epoch: 44 [50000/60000 (83%)]\tLoss: 122.176797\n",
      "====> Epoch: 44 Average loss: 123.7669\n",
      "====> Test set loss: 127.6595\n",
      "Train Epoch: 45 [0/60000 (0%)]\tLoss: 117.036611\n",
      "Train Epoch: 45 [10000/60000 (17%)]\tLoss: 123.748145\n",
      "Train Epoch: 45 [20000/60000 (33%)]\tLoss: 125.057695\n",
      "Train Epoch: 45 [30000/60000 (50%)]\tLoss: 126.956465\n",
      "Train Epoch: 45 [40000/60000 (67%)]\tLoss: 127.917676\n",
      "Train Epoch: 45 [50000/60000 (83%)]\tLoss: 124.566240\n",
      "====> Epoch: 45 Average loss: 123.6394\n",
      "====> Test set loss: 127.7450\n",
      "Train Epoch: 46 [0/60000 (0%)]\tLoss: 120.136836\n",
      "Train Epoch: 46 [10000/60000 (17%)]\tLoss: 121.659385\n",
      "Train Epoch: 46 [20000/60000 (33%)]\tLoss: 119.691885\n",
      "Train Epoch: 46 [30000/60000 (50%)]\tLoss: 120.743926\n",
      "Train Epoch: 46 [40000/60000 (67%)]\tLoss: 120.463477\n",
      "Train Epoch: 46 [50000/60000 (83%)]\tLoss: 126.896992\n",
      "====> Epoch: 46 Average loss: 123.5830\n",
      "====> Test set loss: 127.6290\n",
      "Train Epoch: 47 [0/60000 (0%)]\tLoss: 119.083564\n",
      "Train Epoch: 47 [10000/60000 (17%)]\tLoss: 121.562803\n",
      "Train Epoch: 47 [20000/60000 (33%)]\tLoss: 122.352744\n",
      "Train Epoch: 47 [30000/60000 (50%)]\tLoss: 132.901865\n",
      "Train Epoch: 47 [40000/60000 (67%)]\tLoss: 120.488115\n",
      "Train Epoch: 47 [50000/60000 (83%)]\tLoss: 114.948340\n",
      "====> Epoch: 47 Average loss: 123.4368\n",
      "====> Test set loss: 127.8746\n",
      "Train Epoch: 48 [0/60000 (0%)]\tLoss: 123.406992\n",
      "Train Epoch: 48 [10000/60000 (17%)]\tLoss: 118.386523\n",
      "Train Epoch: 48 [20000/60000 (33%)]\tLoss: 122.758604\n",
      "Train Epoch: 48 [30000/60000 (50%)]\tLoss: 126.962988\n",
      "Train Epoch: 48 [40000/60000 (67%)]\tLoss: 126.874658\n",
      "Train Epoch: 48 [50000/60000 (83%)]\tLoss: 118.320186\n",
      "====> Epoch: 48 Average loss: 123.3683\n",
      "====> Test set loss: 127.7820\n",
      "Train Epoch: 49 [0/60000 (0%)]\tLoss: 118.930039\n",
      "Train Epoch: 49 [10000/60000 (17%)]\tLoss: 117.742520\n",
      "Train Epoch: 49 [20000/60000 (33%)]\tLoss: 130.135820\n",
      "Train Epoch: 49 [30000/60000 (50%)]\tLoss: 124.207959\n",
      "Train Epoch: 49 [40000/60000 (67%)]\tLoss: 119.557070\n",
      "Train Epoch: 49 [50000/60000 (83%)]\tLoss: 126.154053\n",
      "====> Epoch: 49 Average loss: 123.3077\n",
      "====> Test set loss: 127.6654\n",
      "Train Epoch: 50 [0/60000 (0%)]\tLoss: 122.121064\n",
      "Train Epoch: 50 [10000/60000 (17%)]\tLoss: 119.634424\n",
      "Train Epoch: 50 [20000/60000 (33%)]\tLoss: 123.973223\n",
      "Train Epoch: 50 [30000/60000 (50%)]\tLoss: 134.972686\n",
      "Train Epoch: 50 [40000/60000 (67%)]\tLoss: 118.465039\n",
      "Train Epoch: 50 [50000/60000 (83%)]\tLoss: 121.499160\n",
      "====> Epoch: 50 Average loss: 123.2038\n",
      "====> Test set loss: 127.7925\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 51):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated samples for digit 0\n",
      "Generated samples for digit 1\n",
      "Generated samples for digit 2\n",
      "Generated samples for digit 3\n",
      "Generated samples for digit 4\n"
     ]
    }
   ],
   "source": [
    "# Conditional generation function\n",
    "def generate_conditional_samples(digit, num_samples=64):\n",
    "    \"\"\"Generate samples of a specific digit\"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Create random latent vectors\n",
    "        z = torch.randn(num_samples, 2).cuda()\n",
    "        \n",
    "        # Create one-hot encoding for the desired digit\n",
    "        target_labels = torch.zeros(num_samples, 10).cuda()\n",
    "        target_labels[:, digit] = 1.0\n",
    "        \n",
    "        # Generate samples using the decoder\n",
    "        generated_images = vae.decoder(z, target_labels)\n",
    "        \n",
    "        return generated_images\n",
    "\n",
    "# Generate samples for different digits\n",
    "for digit in [0, 1, 2, 3, 4]:\n",
    "    samples = generate_conditional_samples(digit, num_samples=16)\n",
    "    save_image(samples.view(16, 1, 28, 28), f'./samples/digit_{digit}.png')\n",
    "    print(f\"Generated samples for digit {digit}\")\n",
    "\n",
    "# # Also generate some random samples (without specific digit)\n",
    "# with torch.no_grad():\n",
    "#     z = torch.randn(64, 2).cuda()\n",
    "#     random_labels = torch.zeros(64, 10).cuda()\n",
    "#     random_labels[:, 5] = 1.0  # Generate digit 5\n",
    "#     sample = vae.decoder(z, random_labels)\n",
    "#     save_image(sample.view(64, 1, 28, 28), './samples/random_samples.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "homl3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
